{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seminar 6: Recurrent\tNeural\tNetworks and\tNatural\tLanguage\tProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Decoding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfolding a RNN in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Layers in Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing\tOur\tFirst\tRecurrent\tNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow\tsupports\tvarious\tvariants\tof\n",
    "RNNs\tthat\tcan\tbe\tfound\tin\tthe\t `tf.nn.rnn_cell` \tmodule.\tWith\tthe\t `tf.nn.dynamic_rnn()` \toperation,\n",
    "TensorFlow\talso\timplements\tthe\tRNN\tdynamics\tfor\tus.\n",
    "There\tis\talso\ta\tversion\tof\tthis\tfunction\tthat\tadds\tthe\tunfolded\toperations\tto\tthe\tgraph\n",
    "instead\tof\tusing\ta\tloop.\tHowever,\tthis\tconsumes\tconsiderably\tmore\tmemory\tand\thas\tno\n",
    "real\tbenefits.\tWe\ttherefore\tuse\tthe\tnewer\t `dynamic_rnn()` \toperation.\n",
    "As\tparameters,\t `dynamic_rnn()` \ttakes\ta\trecurrent\tnetwork\tdefinition\tand\tthe\tbatch\tof\tinput\n",
    "sequences.\tFor\tnow,\tthe\tsequences\tall\thave\tthe\tsame\tlength.\tThe\tfunction\tcreates\tthe\n",
    "required\tcomputations\tfor\tthe\tRNN\tto\tthe\tcompute\tgraph\tand\treturns\ttwo\ttensors\tholding\n",
    "the\toutputs\tand\thidden\tstates\tat\teach\ttime\tstep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The input data has dimensions batch_size * sequence_length * frame_size.\n",
    "# To not restrict ourselves to a fixed batch size, we use None as size of\n",
    "# the first dimension.\n",
    "sequence_length = 1440\n",
    "frame_size = 10\n",
    "data = tf.placeholder(tf.float32, [None, sequence_length, frame_size])\n",
    "\n",
    "num_neurons = 200\n",
    "network = tf.contrib.rnn.BasicRNNCell(num_neurons)\n",
    "# Define the operations to simulate the RNN for sequence_length steps.\n",
    "outputs, states = tf.nn.dynamic_rnn(network, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    \"\"\"\n",
    "    The movie review dataset is offered by Stanford Universityâ€™s AI department:\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/. It comes as a compressed  tar  archive where\n",
    "    positive and negative reviews can be found as text files in two according folders. We apply\n",
    "    the same pre-processing to the text as in the last section: Extracting plain words using a\n",
    "    regular expression and converting to lower case.\n",
    "    \"\"\"\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = 'imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\tcode\tshould\tbe\tstraight\tforward.\tWe\tjust\tuse\tthe\tvocabulary\tto\tdetermine\tthe\tindex\tof\n",
    "a\tword\tand\tuse\tthat\tindex\tto\tfind\tthe\tright\tembedding\tvector.\tThe\tfollowing\tclass\talso\n",
    "padds\tthe\tsequences\tto\tthe\tsame\tlength\tso\twe\tcan\teasily\tfit\tbatches\tof\tmultiple\treviews\n",
    "into\tyour\tnetwork\tlater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy.en'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-925d13d76162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy.en'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English \n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, length):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "        self.parser = English()\n",
    "        self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence):\n",
    "        data = np.zeros((self._length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence\tLabelling\tModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\twant\tto\tclassify\tthe\tsentiment\tof\ttext\tsequences.\tBecause\tthis\tis\ta\tsupervised\n",
    "setting,\twe\tpass\ttwo\tplaceholders\tto\tthe\tmodel:\tone\tfor\tthe\tinput\t data ,\tor\tthe\tsequence,\tand\n",
    "one\tfor\tthe\t target \tvalue,\tor\tthe\tsentiment.\tWe\talso\tpass\tin\tthe\t params \tobject\tthat\tcontains\n",
    "configuration\tparameters\tlike\tthe\tsize\tof\tthe\trecurrent\tlayer,\tits\tcell\tarchitecture\t(LSTM,\n",
    "GRU,\tetc),\tand\tthe\toptimizer\tto\tuse.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy import lazy\n",
    "\n",
    "class SequenceClassificationModel:\n",
    "    def __init__(self, data, params):\n",
    "        self.params = params\n",
    "        self._create_placeholders()\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.error\n",
    "        self.optimize\n",
    "        self._create_summaries()\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.data = tf.placeholder(tf.float32, [None, self.params.seq_length, self.params.embed_length])\n",
    "            self.target = tf.placeholder(tf.float32, [None, 2])\n",
    "  \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "            tf.summary.scalar('erroe', self.error)\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "    @lazy\n",
    "    def length(self):\n",
    "    # First, we obtain the lengths of sequences in the current data batch. We need this since\n",
    "    # the data comes as a single tensor, padded with zero vectors to the longest review length.\n",
    "    # Instead of keeping track of the sequence lengths of every review, we just compute it\n",
    "    # dynamically in TensorFlow.\n",
    "    \n",
    "        with tf.name_scope(\"seq_length\"):\n",
    "            used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "            length = tf.reduce_sum(used, reduction_indices=1)\n",
    "            length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @lazy\n",
    "    def prediction(self):\n",
    "    # Note that the last relevant output activation of the RNN has a different index for each\n",
    "    # sequence in the training batch. This is because each review has a different length. We\n",
    "    # already know the length of each sequence.\n",
    "    # The problem is that we want to index in the dimension of time steps, which is\n",
    "    # the second dimension in the batch of shape  (sequences, time_steps, word_vectors) .\n",
    "    \n",
    "        with tf.name_scope(\"recurrent_layer\"):\n",
    "            output, _ = tf.nn.dynamic_rnn(\n",
    "                self.params.rnn_cell(self.params.rnn_hidden),\n",
    "                self.data,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=self.length\n",
    "            )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            num_classes = int(self.target.get_shape()[1])\n",
    "            weight = tf.Variable(tf.truncated_normal(\n",
    "                [self.params.rnn_hidden, num_classes], stddev=0.01))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "    \n",
    "    @lazy\n",
    "    def error(self):\n",
    "        self.mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "    \n",
    "    @lazy\n",
    "    def optimize(self):\n",
    "    # RNNs are quite hard to train and weights tend to diverge if the hyper parameters do not\n",
    "    # play nicely together. The idea of gradient clipping is to restrict the the values of the\n",
    "    # gradient to a sensible range. This way, we can limit the maximum weight updates.\n",
    "\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            gradient = self.params.optimizer.compute_gradients(self.cost)\n",
    "            if self.params.gradient_clipping:\n",
    "                limit = self.params.gradient_clipping\n",
    "                gradient = [\n",
    "                    (tf.clip_by_value(g, -limit, limit), v)\n",
    "                    if g is not None else (None, v)\n",
    "                    for g, v in gradient]\n",
    "            optimize = self.params.optimizer.apply_gradients(gradient)\n",
    "        return optimize\n",
    "    \n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        with tf.name_scope(\"last_relevant\"):\n",
    "            # As of now, TensorFlow only supports indexing along the first dimension, using\n",
    "            # tf.gather() . We thus flatten the first two dimensions of the output activations from their\n",
    "            # shape of  sequences x time_steps x word_vectors  and construct an index into this resulting tensor.\n",
    "            batch_size = tf.shape(output)[0]\n",
    "            max_length = int(output.get_shape()[1])\n",
    "            output_size = int(output.get_shape()[2])\n",
    "\n",
    "            # The index takes into account the start indices for each sequence in the flat tensor and adds\n",
    "            # the sequence length to it. Actually, we only add  length - 1  so that we select the last valid\n",
    "            # time step.\n",
    "            index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "            flat = tf.reshape(output, [-1, output_size])\n",
    "            relevant = tf.gather(flat, index)\n",
    "        return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batched(iterator, length, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        data = np.zeros((batch_size, length, embedding.dimensions))\n",
    "        target = np.zeros((batch_size, 2))\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            data[index] = embedding(text)\n",
    "            target[index] = [1, 0] if label else [0, 1]\n",
    "        yield data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7b0453094d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "length = max(len(x[0]) for x in reviews)\n",
    "embedding = Embedding(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attrdict import AttrDict\n",
    "\n",
    "params = AttrDict(\n",
    "    rnn_cell=tf.contrib.rnn.GRUCell,\n",
    "    rnn_hidden=300,\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.002),\n",
    "    batch_size=20,\n",
    "    gradient_clipping=100,\n",
    "    seq_length=length,\n",
    "    embed_length=embedding.dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = preprocess_batched(reviews, length, embedding, params.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SequenceClassificationModel(data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\n",
    ")) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('graphs', sess.graph)\n",
    "    for index, batch in enumerate(batches):\n",
    "        feed = {model.data: batch[0], model.target: batch[1]}\n",
    "        error, _, summary_str = sess.run([model.error, model.optimize, model.summary], feed)\n",
    "        print('{}: {:3.1f}%'.format(index + 1, 100 * error))\n",
    "        if index % 1 == 0:\n",
    "            summary_writer.add_summary(summary_str, index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
