{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cifar10\n",
    "from cifar10 import img_size, num_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: ../data/CIFAR-10/cifar-10-batches-py/batches.meta\n",
      "Loading data: ../data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: ../data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: ../data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: ../data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: ../data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: ../data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size_cropped = 24\n",
    "cifar10.maybe_download_and_extract()\n",
    "class_names = cifar10.load_class_names()\n",
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()\n",
    "len(images_train), len(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name=\"x\")\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name=\"y_true\")\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(image, training):\n",
    "    if training:\n",
    "        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "    else:\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                       target_height=img_size_cropped,\n",
    "                                                       target_width=img_size_cropped)\n",
    "    return image\n",
    "\n",
    "def pre_process(images, training):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        return tf.map_fn(lambda image: pre_process_image(image, training), images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, name=\"weight\"):\n",
    "    return tf.get_variable(name, initializer=tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "\n",
    "def bias_variable(shape, name=\"bias\"):\n",
    "    return tf.get_variable(name, initializer=tf.constant(0.0, dtype=tf.float32, shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, units=3, depth=3, dim=10, stride=1, \n",
    "                 groups=1, padding=\"SAME\", name=\"conv\"):\n",
    "        self.units = units\n",
    "        self.depth = depth\n",
    "        self.dim = dim\n",
    "        self.groups = groups\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope(self.name):\n",
    "            weight = weight_variable([self.units, self.units, \n",
    "                                      self.depth // self.groups, self.dim])\n",
    "            bias = bias_variable([self.dim])\n",
    "            from functools import partial\n",
    "            conv_f = partial(tf.nn.conv2d, \n",
    "                             strides=[1, self.stride, self.stride, 1],\n",
    "                             padding=self.padding)\n",
    "            y = tf.concat([conv_f(x_group, weight_group) for x_group, weight_group in zip(\n",
    "                    tf.split(x, num_or_size_splits=self.groups, axis=3), \n",
    "                    tf.split(weight, num_or_size_splits=self.groups, axis=3))], axis=3)\n",
    "            return tf.nn.relu(tf.nn.bias_add(y, bias))            \n",
    "\n",
    "\n",
    "class LRN:\n",
    "    def __init__(self, depth_radius=2, bias=2, alpha=1e-4, beta=0.75, name=\"lrn\"):\n",
    "        self.depth_radius = depth_radius\n",
    "        self.bias = bias\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope(self.name):\n",
    "            return tf.nn.lrn(x, \n",
    "                             depth_radius=self.depth_radius, \n",
    "                             bias=self.bias, \n",
    "                             alpha=self.alpha, \n",
    "                             name=self.name)\n",
    "    \n",
    "\n",
    "class MaxPool:\n",
    "    def __init__(self, units=2, stride=1, padding=\"SAME\", name=\"maxpool\"):\n",
    "        self.units = units\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope(self.name):\n",
    "            return tf.nn.max_pool(x, \n",
    "                                  ksize=[1, self.units, self.units, 1], \n",
    "                                  strides=[1, self.stride, self.stride, 1], \n",
    "                                  padding=self.padding,\n",
    "                                  name=self.name)\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, dim=10, activation=\"relu\", name=\"dense\"):\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "    \n",
    "    def _activation_f(self, x):\n",
    "        if self.activation is None:\n",
    "            return x\n",
    "        elif self.activation == \"relu\":\n",
    "            return tf.nn.relu(x)\n",
    "        else:\n",
    "            raise ValueError(\"No such activation func.\")\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope(self.name):\n",
    "            units = int(np.prod(x.shape[1:]))\n",
    "            x = tf.reshape(x, [-1, units])\n",
    "            weight = weight_variable([units, self.dim])\n",
    "            bias = bias_variable([self.dim])\n",
    "            y = tf.nn.xw_plus_b(x, weight, bias)\n",
    "            y = self._activation_f(y)\n",
    "            return y\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, prob=0.5, name=\"dropout\"):\n",
    "        self.prob = prob\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope(self.name):\n",
    "            return tf.nn.dropout(x, keep_prob=self.prob, name=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(x, training):\n",
    "    # [24, 24, 3] => [11, 11, 48]\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        x = Conv(units=4, depth=3, dim=48, stride=2)(x)\n",
    "        x = LRN()(x)\n",
    "        x = MaxPool(units=2, padding=\"VALID\")(x)\n",
    "    \n",
    "    # ... => [4, 4, 128]\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        x = Conv(units=3, depth=48, dim=128, stride=2, groups=2)(x)\n",
    "        x = LRN()(x)\n",
    "        x = MaxPool(units=2, padding=\"VALID\")(x)\n",
    "    \n",
    "    # ... => ...\n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        x = Conv(units=3, depth=128, dim=192)(x)\n",
    "    \n",
    "    # ... => ...\n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        x = Conv(units=3, depth=192, dim=192, groups=2)(x)\n",
    "    \n",
    "    # ... => [3, 3, 128]\n",
    "    with tf.variable_scope(\"conv5\"):\n",
    "        x = Conv(units=3, depth=192, dim=128, groups=2)(x)\n",
    "        x = MaxPool(units=3, padding=\"VALID\")(x)\n",
    "    \n",
    "    # ... => [10]\n",
    "    with tf.variable_scope(\"fully_connected\"):\n",
    "        x = Dense(512, name=\"dense1\")(x)\n",
    "        x = Dropout(0.5)(x) if training else x\n",
    "        x = Dense(512, name=\"dense2\")(x)\n",
    "        x = Dropout(0.5)(x) if training else x\n",
    "        x = Dense(10, activation=None, name=\"dense3\")(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(training):\n",
    "    with tf.variable_scope(\"nn\", reuse=not training):\n",
    "        images = x\n",
    "        images = pre_process(images, training)\n",
    "        y_pred = alexnet(images, training)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "        return y_pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network for Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(initial_value=0, name=\"global_step\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loss = create_network(training=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network for Test Phase / Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, _ = create_network(training=False)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3983236015700958866, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6976641434\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 3728143141123001705\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_cpu():\n",
    "    return tf.Session(config=tf.ConfigProto(device_count={\"GPU\": 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_gpu(mem_frac=0.333):\n",
    "    gpu_options = tf.GPUOptions(\n",
    "        per_process_gpu_memory_fraction=mem_frac\n",
    "    )\n",
    "    config = tf.ConfigProto(\n",
    "        gpu_options=gpu_options,\n",
    "        log_device_placement=True,\n",
    "        allow_soft_placement=True\n",
    "    )\n",
    "    session = tf.Session(\n",
    "        config=config\n",
    "    )\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = with_gpu()\n",
    "writer = tf.summary.FileWriter('./graphs', session.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore or initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"checkpoints/\"\n",
    "save_path = os.path.join(save_dir, \"alexnet_task\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/alexnet_task-49999\n",
      "Restored checkpoint from: checkpoints/alexnet_task-49999\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "    saver.restore(session, save_path=last_chk_path)\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to get a random training-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(train_batch_size=64):\n",
    "    num_images = len(images_train)\n",
    "    idx = np.random.choice(num_images, size=train_batch_size, replace=False)\n",
    "    x_batch = images_train[idx, :, :, :]\n",
    "    y_batch = labels_train[idx, :]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The progress is printed every 100 iterations. A checkpoint is saved every 1000 iterations and also after the last iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        x_batch, y_true_batch = random_batch()\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "        i_global, _ = session.run([global_step, optimizer], feed_dict=feed_dict_train)\n",
    "        if (i_global % 100 == 0) or (i_global == num_iterations - 1):\n",
    "            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            msg = \"Global Step: {0:>6}, Training Batch Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(i_global, batch_acc))\n",
    "        if (i_global % 1000 == 0) or (i_global == num_iterations - 1):\n",
    "            saver.save(session, save_path=save_path, global_step=global_step)\n",
    "            print(\"Saved checkpoint.\")\n",
    "        if i_global == num_iterations - 1:\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cls(images, labels, cls_true, batch_size=256):\n",
    "    num_images = len(images)\n",
    "    cls_pred = np.zeros(shape=num_images, dtype=np.int)\n",
    "    i = 0\n",
    "    while i < num_images:\n",
    "        j = min(i + batch_size, num_images)\n",
    "        feed_dict = {x: images[i:j, :], y_true: labels[i:j, :]}\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "        i = j\n",
    "    correct = (cls_true == cls_pred)\n",
    "    return correct, cls_pred\n",
    "\n",
    "\n",
    "def predict_cls_test():\n",
    "    return predict_cls(images=images_test, labels=labels_test, cls_true=cls_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for showing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_accuracy():\n",
    "    correct, cls_pred = predict_cls_test()\n",
    "    acc, num_correct = correct.mean(), correct.sum()\n",
    "    num_images = len(correct)\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, num_correct, num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:  50000, Training Batch Accuracy:  71.9%\n",
      "Saved checkpoint.\n",
      "Global Step:  50100, Training Batch Accuracy:  78.1%\n",
      "Global Step:  50200, Training Batch Accuracy:  67.2%\n",
      "Global Step:  50300, Training Batch Accuracy:  85.9%\n",
      "Global Step:  50400, Training Batch Accuracy:  70.3%\n",
      "Global Step:  50500, Training Batch Accuracy:  70.3%\n",
      "Global Step:  50600, Training Batch Accuracy:  71.9%\n",
      "Global Step:  50700, Training Batch Accuracy:  73.4%\n",
      "Global Step:  50800, Training Batch Accuracy:  75.0%\n",
      "Global Step:  50900, Training Batch Accuracy:  68.8%\n",
      "Global Step:  51000, Training Batch Accuracy:  65.6%\n",
      "Saved checkpoint.\n",
      "Global Step:  51100, Training Batch Accuracy:  78.1%\n",
      "Global Step:  51200, Training Batch Accuracy:  65.6%\n",
      "Global Step:  51300, Training Batch Accuracy:  65.6%\n",
      "Global Step:  51400, Training Batch Accuracy:  71.9%\n",
      "Global Step:  51500, Training Batch Accuracy:  76.6%\n",
      "Global Step:  51600, Training Batch Accuracy:  75.0%\n",
      "Global Step:  51700, Training Batch Accuracy:  73.4%\n",
      "Global Step:  51800, Training Batch Accuracy:  68.8%\n",
      "Global Step:  51900, Training Batch Accuracy:  68.8%\n",
      "Global Step:  52000, Training Batch Accuracy:  73.4%\n",
      "Saved checkpoint.\n",
      "Global Step:  52100, Training Batch Accuracy:  79.7%\n",
      "Global Step:  52200, Training Batch Accuracy:  76.6%\n",
      "Global Step:  52300, Training Batch Accuracy:  73.4%\n",
      "Global Step:  52400, Training Batch Accuracy:  68.8%\n",
      "Global Step:  52500, Training Batch Accuracy:  75.0%\n",
      "Global Step:  52600, Training Batch Accuracy:  76.6%\n",
      "Global Step:  52700, Training Batch Accuracy:  76.6%\n",
      "Global Step:  52800, Training Batch Accuracy:  70.3%\n",
      "Global Step:  52900, Training Batch Accuracy:  79.7%\n",
      "Global Step:  53000, Training Batch Accuracy:  76.6%\n",
      "Saved checkpoint.\n",
      "Global Step:  53100, Training Batch Accuracy:  65.6%\n",
      "Global Step:  53200, Training Batch Accuracy:  68.8%\n",
      "Global Step:  53300, Training Batch Accuracy:  67.2%\n",
      "Global Step:  53400, Training Batch Accuracy:  73.4%\n",
      "Global Step:  53500, Training Batch Accuracy:  67.2%\n",
      "Global Step:  53600, Training Batch Accuracy:  79.7%\n",
      "Global Step:  53700, Training Batch Accuracy:  71.9%\n",
      "Global Step:  53800, Training Batch Accuracy:  71.9%\n",
      "Global Step:  53900, Training Batch Accuracy:  75.0%\n",
      "Global Step:  54000, Training Batch Accuracy:  75.0%\n",
      "Saved checkpoint.\n",
      "Global Step:  54100, Training Batch Accuracy:  71.9%\n",
      "Global Step:  54200, Training Batch Accuracy:  81.2%\n",
      "Global Step:  54300, Training Batch Accuracy:  76.6%\n",
      "Global Step:  54400, Training Batch Accuracy:  76.6%\n",
      "Global Step:  54500, Training Batch Accuracy:  71.9%\n",
      "Global Step:  54600, Training Batch Accuracy:  76.6%\n",
      "Global Step:  54700, Training Batch Accuracy:  67.2%\n",
      "Global Step:  54800, Training Batch Accuracy:  81.2%\n",
      "Global Step:  54900, Training Batch Accuracy:  76.6%\n",
      "Global Step:  55000, Training Batch Accuracy:  71.9%\n",
      "Saved checkpoint.\n",
      "Global Step:  55100, Training Batch Accuracy:  68.8%\n",
      "Global Step:  55200, Training Batch Accuracy:  70.3%\n",
      "Global Step:  55300, Training Batch Accuracy:  68.8%\n",
      "Global Step:  55400, Training Batch Accuracy:  71.9%\n",
      "Global Step:  55500, Training Batch Accuracy:  62.5%\n",
      "Global Step:  55600, Training Batch Accuracy:  76.6%\n",
      "Global Step:  55700, Training Batch Accuracy:  71.9%\n",
      "Global Step:  55800, Training Batch Accuracy:  73.4%\n",
      "Global Step:  55900, Training Batch Accuracy:  78.1%\n",
      "Global Step:  56000, Training Batch Accuracy:  76.6%\n",
      "Saved checkpoint.\n",
      "Global Step:  56100, Training Batch Accuracy:  67.2%\n",
      "Global Step:  56200, Training Batch Accuracy:  73.4%\n",
      "Global Step:  56300, Training Batch Accuracy:  71.9%\n",
      "Global Step:  56400, Training Batch Accuracy:  75.0%\n",
      "Global Step:  56500, Training Batch Accuracy:  64.1%\n",
      "Global Step:  56600, Training Batch Accuracy:  78.1%\n",
      "Global Step:  56700, Training Batch Accuracy:  68.8%\n",
      "Global Step:  56800, Training Batch Accuracy:  78.1%\n",
      "Global Step:  56900, Training Batch Accuracy:  73.4%\n",
      "Global Step:  57000, Training Batch Accuracy:  75.0%\n",
      "Saved checkpoint.\n",
      "Global Step:  57100, Training Batch Accuracy:  78.1%\n",
      "Global Step:  57200, Training Batch Accuracy:  70.3%\n",
      "Global Step:  57300, Training Batch Accuracy:  78.1%\n",
      "Global Step:  57400, Training Batch Accuracy:  81.2%\n",
      "Global Step:  57500, Training Batch Accuracy:  73.4%\n",
      "Global Step:  57600, Training Batch Accuracy:  78.1%\n",
      "Global Step:  57700, Training Batch Accuracy:  71.9%\n",
      "Global Step:  57800, Training Batch Accuracy:  79.7%\n",
      "Global Step:  57900, Training Batch Accuracy:  71.9%\n",
      "Global Step:  58000, Training Batch Accuracy:  81.2%\n",
      "Saved checkpoint.\n",
      "Global Step:  58100, Training Batch Accuracy:  70.3%\n",
      "Global Step:  58200, Training Batch Accuracy:  73.4%\n",
      "Global Step:  58300, Training Batch Accuracy:  70.3%\n",
      "Global Step:  58400, Training Batch Accuracy:  73.4%\n",
      "Global Step:  58500, Training Batch Accuracy:  87.5%\n",
      "Global Step:  58600, Training Batch Accuracy:  84.4%\n",
      "Global Step:  58700, Training Batch Accuracy:  76.6%\n",
      "Global Step:  58800, Training Batch Accuracy:  68.8%\n",
      "Global Step:  58900, Training Batch Accuracy:  82.8%\n",
      "Global Step:  59000, Training Batch Accuracy:  81.2%\n",
      "Saved checkpoint.\n",
      "Global Step:  59100, Training Batch Accuracy:  67.2%\n",
      "Global Step:  59200, Training Batch Accuracy:  81.2%\n",
      "Global Step:  59300, Training Batch Accuracy:  78.1%\n",
      "Global Step:  59400, Training Batch Accuracy:  71.9%\n",
      "Global Step:  59500, Training Batch Accuracy:  76.6%\n",
      "Global Step:  59600, Training Batch Accuracy:  75.0%\n",
      "Global Step:  59700, Training Batch Accuracy:  81.2%\n",
      "Global Step:  59800, Training Batch Accuracy:  65.6%\n",
      "Global Step:  59900, Training Batch Accuracy:  75.0%\n",
      "Global Step:  60000, Training Batch Accuracy:  78.1%\n",
      "Saved checkpoint.\n",
      "Global Step:  60100, Training Batch Accuracy:  76.6%\n",
      "Global Step:  60200, Training Batch Accuracy:  71.9%\n",
      "Global Step:  60300, Training Batch Accuracy:  71.9%\n",
      "Global Step:  60400, Training Batch Accuracy:  76.6%\n",
      "Global Step:  60500, Training Batch Accuracy:  75.0%\n",
      "Global Step:  60600, Training Batch Accuracy:  70.3%\n",
      "Global Step:  60700, Training Batch Accuracy:  73.4%\n",
      "Global Step:  60800, Training Batch Accuracy:  84.4%\n",
      "Global Step:  60900, Training Batch Accuracy:  75.0%\n",
      "Global Step:  61000, Training Batch Accuracy:  70.3%\n",
      "Saved checkpoint.\n",
      "Global Step:  61100, Training Batch Accuracy:  65.6%\n",
      "Global Step:  61200, Training Batch Accuracy:  78.1%\n",
      "Global Step:  61300, Training Batch Accuracy:  75.0%\n",
      "Global Step:  61400, Training Batch Accuracy:  71.9%\n",
      "Global Step:  61500, Training Batch Accuracy:  62.5%\n",
      "Global Step:  61600, Training Batch Accuracy:  75.0%\n",
      "Global Step:  61700, Training Batch Accuracy:  60.9%\n",
      "Global Step:  61800, Training Batch Accuracy:  78.1%\n",
      "Global Step:  61900, Training Batch Accuracy:  78.1%\n",
      "Global Step:  62000, Training Batch Accuracy:  78.1%\n",
      "Saved checkpoint.\n",
      "Global Step:  62100, Training Batch Accuracy:  71.9%\n",
      "Global Step:  62200, Training Batch Accuracy:  71.9%\n",
      "Global Step:  62300, Training Batch Accuracy:  81.2%\n",
      "Global Step:  62400, Training Batch Accuracy:  78.1%\n",
      "Global Step:  62500, Training Batch Accuracy:  71.9%\n",
      "Global Step:  62600, Training Batch Accuracy:  78.1%\n",
      "Global Step:  62700, Training Batch Accuracy:  76.6%\n",
      "Global Step:  62800, Training Batch Accuracy:  87.5%\n",
      "Global Step:  62900, Training Batch Accuracy:  73.4%\n",
      "Global Step:  63000, Training Batch Accuracy:  84.4%\n",
      "Saved checkpoint.\n",
      "Global Step:  63100, Training Batch Accuracy:  71.9%\n",
      "Global Step:  63200, Training Batch Accuracy:  82.8%\n",
      "Global Step:  63300, Training Batch Accuracy:  65.6%\n",
      "Global Step:  63400, Training Batch Accuracy:  73.4%\n",
      "Global Step:  63500, Training Batch Accuracy:  81.2%\n",
      "Global Step:  63600, Training Batch Accuracy:  82.8%\n",
      "Global Step:  63700, Training Batch Accuracy:  73.4%\n",
      "Global Step:  63800, Training Batch Accuracy:  76.6%\n",
      "Global Step:  63900, Training Batch Accuracy:  82.8%\n",
      "Global Step:  64000, Training Batch Accuracy:  78.1%\n",
      "Saved checkpoint.\n",
      "Global Step:  64100, Training Batch Accuracy:  75.0%\n",
      "Global Step:  64200, Training Batch Accuracy:  78.1%\n",
      "Global Step:  64300, Training Batch Accuracy:  75.0%\n",
      "Global Step:  64400, Training Batch Accuracy:  75.0%\n",
      "Global Step:  64500, Training Batch Accuracy:  70.3%\n",
      "Global Step:  64600, Training Batch Accuracy:  84.4%\n",
      "Global Step:  64700, Training Batch Accuracy:  71.9%\n",
      "Global Step:  64800, Training Batch Accuracy:  82.8%\n",
      "Global Step:  64900, Training Batch Accuracy:  64.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:  65000, Training Batch Accuracy:  78.1%\n",
      "Saved checkpoint.\n",
      "Global Step:  65100, Training Batch Accuracy:  82.8%\n",
      "Global Step:  65200, Training Batch Accuracy:  68.8%\n",
      "Global Step:  65300, Training Batch Accuracy:  78.1%\n",
      "Global Step:  65400, Training Batch Accuracy:  75.0%\n",
      "Global Step:  65500, Training Batch Accuracy:  76.6%\n",
      "Global Step:  65600, Training Batch Accuracy:  89.1%\n",
      "Global Step:  65700, Training Batch Accuracy:  76.6%\n",
      "Global Step:  65800, Training Batch Accuracy:  79.7%\n",
      "Global Step:  65900, Training Batch Accuracy:  76.6%\n",
      "Global Step:  66000, Training Batch Accuracy:  84.4%\n",
      "Saved checkpoint.\n",
      "Global Step:  66100, Training Batch Accuracy:  82.8%\n",
      "Global Step:  66200, Training Batch Accuracy:  79.7%\n",
      "Global Step:  66300, Training Batch Accuracy:  75.0%\n",
      "Global Step:  66400, Training Batch Accuracy:  76.6%\n",
      "Global Step:  66500, Training Batch Accuracy:  71.9%\n",
      "Global Step:  66600, Training Batch Accuracy:  65.6%\n",
      "Global Step:  66700, Training Batch Accuracy:  78.1%\n",
      "Global Step:  66800, Training Batch Accuracy:  75.0%\n",
      "Global Step:  66900, Training Batch Accuracy:  73.4%\n",
      "Global Step:  67000, Training Batch Accuracy:  65.6%\n",
      "Saved checkpoint.\n",
      "Global Step:  67100, Training Batch Accuracy:  81.2%\n",
      "Global Step:  67200, Training Batch Accuracy:  78.1%\n",
      "Global Step:  67300, Training Batch Accuracy:  68.8%\n",
      "Global Step:  67400, Training Batch Accuracy:  78.1%\n",
      "Global Step:  67500, Training Batch Accuracy:  79.7%\n",
      "Global Step:  67600, Training Batch Accuracy:  76.6%\n",
      "Global Step:  67700, Training Batch Accuracy:  78.1%\n",
      "Global Step:  67800, Training Batch Accuracy:  76.6%\n",
      "Global Step:  67900, Training Batch Accuracy:  70.3%\n",
      "Global Step:  68000, Training Batch Accuracy:  79.7%\n",
      "Saved checkpoint.\n",
      "Global Step:  68100, Training Batch Accuracy:  90.6%\n",
      "Global Step:  68200, Training Batch Accuracy:  75.0%\n",
      "Global Step:  68300, Training Batch Accuracy:  78.1%\n",
      "Global Step:  68400, Training Batch Accuracy:  82.8%\n",
      "Global Step:  68500, Training Batch Accuracy:  84.4%\n",
      "Global Step:  68600, Training Batch Accuracy:  78.1%\n",
      "Global Step:  68700, Training Batch Accuracy:  78.1%\n",
      "Global Step:  68800, Training Batch Accuracy:  82.8%\n",
      "Global Step:  68900, Training Batch Accuracy:  71.9%\n",
      "Global Step:  69000, Training Batch Accuracy:  81.2%\n",
      "Saved checkpoint.\n",
      "Global Step:  69100, Training Batch Accuracy:  73.4%\n",
      "Global Step:  69200, Training Batch Accuracy:  73.4%\n",
      "Global Step:  69300, Training Batch Accuracy:  78.1%\n",
      "Global Step:  69400, Training Batch Accuracy:  73.4%\n",
      "Global Step:  69500, Training Batch Accuracy:  81.2%\n",
      "Global Step:  69600, Training Batch Accuracy:  84.4%\n",
      "Global Step:  69700, Training Batch Accuracy:  78.1%\n",
      "Global Step:  69800, Training Batch Accuracy:  82.8%\n",
      "Global Step:  69900, Training Batch Accuracy:  73.4%\n",
      "Global Step:  70000, Training Batch Accuracy:  68.8%\n",
      "Saved checkpoint.\n",
      "Global Step:  70100, Training Batch Accuracy:  65.6%\n",
      "Global Step:  70200, Training Batch Accuracy:  79.7%\n",
      "Global Step:  70300, Training Batch Accuracy:  73.4%\n",
      "Global Step:  70400, Training Batch Accuracy:  84.4%\n",
      "Global Step:  70500, Training Batch Accuracy:  78.1%\n",
      "Global Step:  70600, Training Batch Accuracy:  76.6%\n",
      "Global Step:  70700, Training Batch Accuracy:  79.7%\n",
      "Global Step:  70800, Training Batch Accuracy:  71.9%\n",
      "Global Step:  70900, Training Batch Accuracy:  73.4%\n",
      "Global Step:  71000, Training Batch Accuracy:  70.3%\n",
      "Saved checkpoint.\n",
      "Global Step:  71100, Training Batch Accuracy:  78.1%\n",
      "Global Step:  71200, Training Batch Accuracy:  82.8%\n",
      "Global Step:  71300, Training Batch Accuracy:  75.0%\n",
      "Global Step:  71400, Training Batch Accuracy:  73.4%\n",
      "Global Step:  71500, Training Batch Accuracy:  71.9%\n",
      "Global Step:  71600, Training Batch Accuracy:  79.7%\n",
      "Global Step:  71700, Training Batch Accuracy:  73.4%\n",
      "Global Step:  71800, Training Batch Accuracy:  82.8%\n",
      "Global Step:  71900, Training Batch Accuracy:  68.8%\n",
      "Global Step:  72000, Training Batch Accuracy:  75.0%\n",
      "Saved checkpoint.\n",
      "Global Step:  72100, Training Batch Accuracy:  78.1%\n",
      "Global Step:  72200, Training Batch Accuracy:  79.7%\n",
      "Global Step:  72300, Training Batch Accuracy:  78.1%\n",
      "Global Step:  72400, Training Batch Accuracy:  68.8%\n",
      "Global Step:  72500, Training Batch Accuracy:  70.3%\n",
      "Global Step:  72600, Training Batch Accuracy:  71.9%\n",
      "Global Step:  72700, Training Batch Accuracy:  76.6%\n",
      "Global Step:  72800, Training Batch Accuracy:  70.3%\n",
      "Global Step:  72900, Training Batch Accuracy:  70.3%\n",
      "Global Step:  73000, Training Batch Accuracy:  73.4%\n",
      "Saved checkpoint.\n",
      "Global Step:  73100, Training Batch Accuracy:  76.6%\n",
      "Global Step:  73200, Training Batch Accuracy:  87.5%\n",
      "Global Step:  73300, Training Batch Accuracy:  75.0%\n",
      "Global Step:  73400, Training Batch Accuracy:  71.9%\n",
      "Global Step:  73500, Training Batch Accuracy:  79.7%\n",
      "Global Step:  73600, Training Batch Accuracy:  79.7%\n",
      "Global Step:  73700, Training Batch Accuracy:  78.1%\n",
      "Global Step:  73800, Training Batch Accuracy:  79.7%\n",
      "Global Step:  73900, Training Batch Accuracy:  76.6%\n",
      "Global Step:  74000, Training Batch Accuracy:  70.3%\n",
      "Saved checkpoint.\n",
      "Global Step:  74100, Training Batch Accuracy:  79.7%\n",
      "Global Step:  74200, Training Batch Accuracy:  76.6%\n",
      "Global Step:  74300, Training Batch Accuracy:  78.1%\n",
      "Global Step:  74400, Training Batch Accuracy:  70.3%\n",
      "Global Step:  74500, Training Batch Accuracy:  78.1%\n",
      "Global Step:  74600, Training Batch Accuracy:  82.8%\n",
      "Global Step:  74700, Training Batch Accuracy:  84.4%\n",
      "Global Step:  74800, Training Batch Accuracy:  84.4%\n",
      "Global Step:  74900, Training Batch Accuracy:  81.2%\n",
      "Global Step:  74999, Training Batch Accuracy:  73.4%\n",
      "Saved checkpoint.\n",
      "Time usage: 0:22:49\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=75 * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 73.9% (7385 / 10000)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
